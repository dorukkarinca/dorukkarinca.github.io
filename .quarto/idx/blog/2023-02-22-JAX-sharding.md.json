{"title":"How to split a large model on Colab TPUs","markdown":{"yaml":{"title":"How to split a large model on Colab TPUs","date":"2023-02-22","permalink":"/blog/jax-sharding-intro/"},"headingText":"Getting started","containsRefs":false,"markdown":"\n\nGoogle Colab has been an amazing tool for trying out new ML ideas. While pipelines are written on PyTorch or TensorFlow with GPU support, I've been recently interested in Colab's TPU accelerator offering and the JAX platform.\n\n[Colab offers 64 GiB of high-bandwidth memory](https://jannik-zuern.medium.com/using-a-tpu-in-google-colab-54257328d7da#:~:text=While%20the%20Tesla%20K80%20available,High%20Bandwidth%20Memory%20(HBM).) with a [TPUv2](https://forums.fast.ai/t/google-colab-quitely-turn-on-tpu-v2-for-free-to-everyone/23329) version -- a considerable jump in memory offering compared to Colab's GPU instances. (More details about different TPU versions is available at [Google Cloud](https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#tpu_v2).) Let's see if we can run the full [stable diffusion model by Runway](https://github.com/runwayml/stable-diffusion) with the [HuggingFace backbone](https://github.com/huggingface/diffusers) using the provided Jax + TPU support.\n\n\nHuggingFace offers a convenient starting point for Jax + TPU:\n\n```python\nimport jax\nimport numpy as np\nfrom flax.jax_utils import replicate\nfrom flax.training.common_utils import shard\n\nfrom diffusers import FlaxStableDiffusionPipeline\n\ndtype = jnp.bfloat16\n\npipeline, params = FlaxStableDiffusionPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", revision=\"bf16\", dtype=dtype, from_pt=True\n)\nprompt = \"A cinematic film still of Morgan Freeman starring as Jimi Hendrix, portrait, 40mm lens, shallow depth of field, close up, split lighting, cinematic\"\nprompt = [prompt] * jax.device_count()\nprompt_ids = pipeline.prepare_inputs(prompt)\n\np_params = replicate(params, devices=jax.devices()) # replicate adds a leading (1, ...) on each tensor\nprompt_ids = shard(prompt_ids)\n\ndef create_key(seed=0):\n    return jax.random.PRNGKey(seed)\n\nrng = create_key(0)\nrng = jax.random.split(rng, jax.device_count())\n\nimages = pipeline(prompt_ids, p_params, rng, jit=True)[0]\nimages = images.reshape((images.shape[0] * images.shape[1], ) + images.shape[-3:])\nimages = pipeline.numpy_to_pil(images)\n\ndef image_grid(imgs, rows, cols):\n    w,h = imgs[0].size\n    grid = Image.new('RGB', size=(cols*w, rows*h))\n    for i, img in enumerate(imgs): grid.paste(img, box=(i%cols*w, i//cols*h))\n    return grid\n  \nimage_grid(images, 2, 4)\n```\n\nIt uses the `replicate` and `shard` utilities of Flax to make 8 copies of the model (one for each TPU device) and parallelize 8 instances of the forward pass. This fits into memory on the smaller, `bfloat16` branch where the pipeline is initialized with `revision=\"bf16\"` and when the safety checker is turned off.\n\nHowever, this block of code runs out of TPU memory for either one of `revision=\"bf16\"` or `revision=\"flax\"`.\n\n```output\nXlaRuntimeError: RESOURCE_EXHAUSTED: Could not allocate 12582912 bytes in memory 0x0x0_HBM0; 12517376 bytes allocatable, 18661376 bytes available\n```\n\nThe only way to get results is to reduce the total allocation size by disabling the safety checker.\n\n```output\nYou have passed `None` for safety_checker to disable its functionality in <class 'diffusers.pipelines.stable_diffusion.pipeline_flax_stable_diffusion.FlaxStableDiffusionPipeline'>. Note that this might lead to problems when using <class 'diffusers.pipelines.stable_diffusion.pipeline_flax_stable_diffusion.FlaxStableDiffusionPipeline'> and is not recommended.\nSome weights of the model checkpoint at stable-diffusion-v1-5/text_encoder were not used when initializing FlaxCLIPTextModel: {('text_model', 'embeddings', 'position_ids')}\n- This IS expected if you are initializing FlaxCLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing FlaxCLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nYou have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_flax_stable_diffusion.FlaxStableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at [https://github.com/huggingface/diffusers/pull/254](https://github.com/huggingface/diffusers/pull/254) .\n```\n![Stable diffusion output](assets/stable_diffusion_output_multi_0.png)\n\nSince the safety checker runs separately from the U-Net, we could load the safety checker on the next TPU core with a small modification in the pipeline. This solves the problem for our specific case but is not generalizable for models that span more than one TPU device. Can we figure out a generalizable way of spreading tensors over multiple TPU devices and still support matrix operations across device borders?\n\nTo motivate this venture, we should keep in mind Colab is offering us a total 64 GiB of high-bandwidth memory spread over 8 devices (or 4 cores). The ability to split a model into multiple devices without making assumptions about the underlying deep learning architecture can be a nifty way to play around with large models with minimal effort.\n\n## JAX support for cross-device programming\n\nJAX tells us if we have two tensors between devices A and B, matrix multiplication fails.\n\n```python\nkey = jax.random.PRNGKey(0)\na = jax.random.uniform(key, shape=(3,5))\nb = jax.random.uniform(key, shape=(5,3))\na = jax.device_put(a, device=jax.devices()[0])\nb = jax.device_put(b, device=jax.devices()[1])\na @ b\n```\n\n```output\nValueError: primitive arguments must be colocated on the same device, got TPU_0(host=0,(0,0,0,0)), TPU_1(host=0,(0,0,0,1))\n```\n\nThe most performance-conscious way to spread a matrix multiplication over two devices is to use JAX's `pjit` library [following its detailed official walkthrough](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html). `pjit`, as of writing this post in February 2023, is an experimental library that optimizes your code with just-in-time compilation and works with a device mesh definition. This device mesh allows the user to spread selected steps of the computation graph across device slices with APIs like `jax.lax.with_sharding_constraint`. An even better [tutorial](https://irhum.github.io/blog/pjit/) shows clear diagrams that show how your matrix multiplication will be sharded for the constraints you impose. The paper mentioned tutorial has even provided an optimal sharding spec for the specific architecture.\n\nUnfortunately, Colab seems to run [a legacy version](https://github.com/google/flax/issues/2263#issuecomment-1173424293) of [TPUv2](https://github.com/google/jax/issues/8300#issuecomment-948458082) which does not have pjit support. This is also a warning at the top of the aforementioned walkthrough. Moreover, producing a `PartitionSpec` and adding `jax.lax.with_sharding_constraint` to individual steps of the computation graph is something that needs to be done manually and correctly for each different deep learning architecture, which wouldn't make our solution generalizable. Fortunately, there's one more thing to try which we find out after a bit of digging into the JAX API.\n\nAlthough `jax.Array` is the default the return type of most JAX operations, `jax.device_put_sharded` and `jax.device_put_replicated` return a special subtype called `ShardedDeviceArray`. and this particular type allows operations between devices. Let's revisit our previous matrix multiplication `a @ b`, this time using `jax.device_put_sharded`:\n\n```python\nkey = jax.random.PRNGKey(0)\na = jax.random.uniform(key, shape=(3,5))\nb = jax.random.uniform(key, shape=(5,3))\na = jax.device_put_sharded([a], jax.devices()[0:1])\nb = jax.device_put_sharded([b], jax.devices()[1:2])\na[0] @ b[0]\n```\n\n```output\nDeviceArray([[2.1039276 , 1.9485016 , 1.3808594 ],\n             [1.0550652 , 0.8924408 , 0.4976406 ],\n             [1.4164448 , 1.3743296 , 0.80164933]], dtype=float32)\n```\n\nSo converting everything to this data type can be used to perform operations across devices. Neat!\n\nOne thing to note is that `jax.device_put_sharded` performs sharding across the first axis so we needed to add a leading `(1, ...)` to each array dimension in order to put whole tensors into a single device. If needed, we can zero-index out this extra dimension and we can also easily convert all tensors in a parameter pytree `params`:\n\n```python\njax.tree_util.tree_map(\n    lambda x: jax.device_put_sharded([x], devices=[random.choice(jax.devices())]),\n    params)\n```\n\nDon't worry, `random.choice` is there for dramatic effect! (though, for model sizes that do not push to the TPU limit it may work) Instead of randomly choosing the allocation device, it makes more intuitive sense to allocate related tensors physically close to each other by developing a clock algorithm that moves the target device pointer to the neighbor when we get close to capacity in the current device.\n\n## Putting it all together\n\nA small performance upgrade is needed on the pipeline side to make it work: while reading the PyTorch model off the disk and converting it into JAX, there's a brief loading region where PyTorch and JAX weights are both referenced in RAM. This duplication of weights exhausts CPU memory before we can move things to the TPU. The quick solution is to keep overwriting the same `state` variable name during conversion so that only one copy of the weights is referenced. The garbage collector does the rest of the heavy-lifting:\n\n```\nAround line 406, change\n\n\t# Step 1: Get the pytorch file\n\tpytorch_model_file = load_state_dict(model_file)\n\t\n\t# Step 2: Convert the weights\n\tstate = convert_pytorch_state_dict_to_flax(pytorch_model_file, model)\n\nwith \n\n\t# Step 1: Get the pytorch file\n\tstate = load_state_dict(model_file)\n\t\n\t# Step 2: Convert the weights\n\tstate = convert_pytorch_state_dict_to_flax(state, model)\n```\n\nFor personal convenicence, I cloned the weight repo (`runwayml/stable-diffusion-v1-5`) to my local relative Google Drive path so my pipeline calls read `FlaxStableDiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5\", ...)` instead.\n\nMost importantly, let's remove the model replication from the HuggingFace starter. After all, the goal is to get a large model to generate a single output as opposed to get a small model to produce multiple outputs.\n\nWith all this in mind, here's the failing case that puts everything on one TPU device, exhausting TPU memory during the forward pass as shown earlier. This still exhausts TPU memory:\n```python\n# single image, jitted and everything on single TPU with safety checker\n\nif \"/content/drive/MyDrive/projects/diffusers/src\" not in sys.path:\n  sys.path.append(\"/content/drive/MyDrive/projects/diffusers/src\")\nfrom diffusers import FlaxStableDiffusionPipeline\n\ndtype = jnp.bfloat16\n\npipeline, params = FlaxStableDiffusionPipeline.from_pretrained(\n    \"stable-diffusion-v1-5\", revision=\"flax\", dtype=dtype, from_pt=True\n)\nprint(\"pipeline loaded\")\nprompt = \"A cinematic film still of Morgan Freeman starring as Jimi Hendrix, portrait, 40mm lens, shallow depth of field, close up, split lighting, cinematic\"\nprompt = [prompt] * 1\nprompt_ids = pipeline.prepare_inputs(prompt)\n\np_params = replicate(params, devices=jax.devices()[2:3]) # replicate adds a leading (1, ...) on each tensor\nprompt_ids = prompt_ids.reshape((1, -1, *prompt_ids.shape[1:])) # fake shard(...)\n\ndef create_key(seed=0):\n    return jax.random.PRNGKey(seed)\nrng = create_key(0)\nrng = jax.random.split(rng, 1)\n\nimages = pipeline(prompt_ids, p_params, rng, jit=True)[0][0]\nimages = pipeline.numpy_to_pil(images)\n\ndef image_grid(imgs, rows, cols):\n    w,h = imgs[0].size\n    grid = Image.new('RGB', size=(cols*w, rows*h))\n    for i, img in enumerate(imgs): grid.paste(img, box=(i%cols*w, i//cols*h))\n    return grid\n\nimage_grid(images, 1, 1)\n```\n\n```output\nXlaRuntimeError: RESOURCE_EXHAUSTED: Could not allocate 1024 bytes in memory 0x0x0_HBM0; 0 bytes allocatable, 0 \nbytes available\n```\n\n\nIf we keep everything the same but shard the model instead, we are in good shape:\n```python\n# single image, jitted, greedily sharded\n\nif \"/content/drive/MyDrive/projects/diffusers/src\" not in sys.path:\n  sys.path.append(\"/content/drive/MyDrive/projects/diffusers/src\")\nfrom diffusers import FlaxStableDiffusionPipeline\n\ndtype = jnp.bfloat16\n\npipeline, params = FlaxStableDiffusionPipeline.from_pretrained(\n    \"stable-diffusion-v1-5\", revision=\"flax\", dtype=dtype, from_pt=True,\n    safety_checker=None\n)\nprompt = \"A cinematic film still of Morgan Freeman starring as Jimi Hendrix, portrait, 40mm lens, shallow depth of field, close up, split lighting, cinematic\"\nprompt = [prompt] * 1\nprompt_ids = pipeline.prepare_inputs(prompt)\n\nprompt_ids = prompt_ids.reshape((1, -1, *prompt_ids.shape[1:])) # fake shard(...)\ncapacities = [4*1e9 for _ in range(len(jax.devices()[2:8]))] # I set it to 4 GB instead of 7.5 GB for extra safety and more aggressive sharding\ndistributor = TensorDistributor(devices=jax.devices()[2:8], capacities=capacities)\np_params = distributor.greedily_distribute_tensors(params, squeeze_first_axis=False) # the first axis aids in jitting\n\ndef create_key(seed=0):\n    return jax.random.PRNGKey(seed)\nrng = create_key(0)\nrng = jax.random.split(rng, 1)\n\nimages = pipeline(prompt_ids, p_params, rng, jit=True)[0][0]\nimages = pipeline.numpy_to_pil(images)\n\ndef image_grid(imgs, rows, cols):\n    w,h = imgs[0].size\n    grid = Image.new('RGB', size=(cols*w, rows*h))\n    for i, img in enumerate(imgs): grid.paste(img, box=(i%cols*w, i//cols*h))\n    return grid\n\nimage_grid(images, 1, 1)\n```\n\nOutput:\n![Stable diffusion output](assets/stable_diffusion_output_1.png)\n\nHere's the definition of `TensorDistributor`, which is a wrapper for the `tree_map` shown earlier but it keeps a tally of TPU capacities to figure out where to allocate:\n```python\nclass TensorDistributor:\n  def __init__(self, devices, capacities):\n    assert len(devices) >= 1, \"At least one device is needed.\"\n    assert len(devices) == len(capacities), \"Devices and capacities must match.\"\n    self.devices, self.capacities = devices, capacities\n    self.idx = len(devices)-1 # index of current allocation device\n  \n  @staticmethod\n  def _move(tensor, device, squeeze_first_axis=False):\n    result = jax.device_put_sharded([tensor], devices=[device])\n    return result[0] if squeeze_first_axis else result\n\n  @staticmethod\n  def randomly_distribute_tensors(\n      params: frozen_dict.FrozenDict,\n      squeeze_first_axis=False,\n      devices=jax.devices()\n  ):\n      \"\"\"Spreads all tensors in `params` across jax.devices() randomly\n      \n      Args:\n          params (dict): Params dict for the network.\n      Returns:\n          new_params: A dictionary identical to params in structure, \n            except tensors are distributed to different devices.\n      \"\"\"\n\n      map = jax.tree_util.tree_map(\n              lambda x: TensorDistributor._move(x, random.choice(devices), \n                                                squeeze_first_axis),\n              params\n            )\n      return map\n\n  def _move_greedy(self, tensor, squeeze_first_axis=False):\n    tensor_size = tensor.nbytes\n    if self.capacities[self.idx] >= tensor_size:\n      self.capacities[self.idx] -= tensor_size\n      return TensorDistributor._move(tensor, self.devices[self.idx], \n                                     squeeze_first_axis)\n\n    # find a new device starting from the current device, allowing wrap-around\n    for i in range(self.idx-1, self.idx-1-len(self.devices), -1):\n      if i < 0:\n        i += len(self.devices) \n      if self.capacities[i] >= tensor_size:\n        self.idx = i\n        self.capacities[self.idx] -= tensor_size\n        return TensorDistributor._move(tensor, self.devices[self.idx],\n                                       squeeze_first_axis)\n    else:\n      raise Exception((f\"Failed to allocate {tensor.nbytes} bytes because the \"\n                       f\"devices have {self.capacities} bytes free.\"))\n  \n  def greedily_distribute_tensors(self,\n      params: frozen_dict.FrozenDict,\n      squeeze_first_axis=False\n  ):\n      \"\"\"Spreads all tensors in `params` across jax.devices() in device order, \n      respecting memory limits set forth by `self.capacity`.\n      \n      Args:\n          params (dict): Params dict for the network.\n      Returns:\n          new_params: A dictionary identical to params in structure, except\n            tensors are distributed to different devices.\n      \"\"\"\n      \n      map = jax.tree_util.tree_map(\n              lambda x: self._move_greedy(x, squeeze_first_axis),\n              params\n            )\n      return map\n```\n*Disclaimer: Feel free to use TensorDistributor according to the MIT License. No warranties are implied.*\n\nWe could have avoided defining `capacities` if we had a function that returns the free memory in bytes i.e. something like `torch.cuda.memory_reserved(0) - torch.cuda.memory_allocated(0)` but for TPUs in JAX. However, the only way I could find to check free memory usage in JAX is to [use a profiler](https://jax.readthedocs.io/en/latest/device_memory_profiling.html) which only outputs `prof` files. Not a big deal since hard-coding capacities seems to be sufficient for the purpose, and also gives the user intentional control over limiting memory consumption per TPU.\n\nHope you find this interesting and useful for supercharging your DL projects using TPUs!"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"output-file":"2023-02-22-JAX-sharding.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.335","theme":"cosmo","title":"How to split a large model on Colab TPUs","date":"2023-02-22","permalink":"/blog/jax-sharding-intro/"},"extensions":{"book":{"multiFile":true}}}}}